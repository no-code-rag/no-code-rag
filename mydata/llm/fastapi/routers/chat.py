# chat.py
import os
import json
import httpx
import asyncio
import logging
import re
import time
from pathlib import Path
from typing import List, Optional, Callable, Awaitable
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from .chat_room import save_streamed_message

router = APIRouter(prefix="/v1/chat")
logging.basicConfig(level=logging.INFO)

LLM_HOST = os.getenv("LLM_HOST", "http://llama:8000")
SEARCH_API_URL = "http://vector:8000/embed_search"
PROMPT_DIR = Path("/mydata/llm/fastapi/config/prompts")
RAG_PROMPT_DIR = Path("/mydata/llm/fastapi/config/rag_prompt")
GLOBAL_CONFIG_PATH = Path("/mydata/llm/fastapi/config/global_config.json")

# åŒæ™‚ãƒ­ãƒ¼ãƒ‰é˜²æ­¢
_model_load_lock = asyncio.Lock()
# æ¨è«–é †åºä¿è¨¼ç”¨ï¼ˆã‚¢ãƒ—ãƒªå´ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºï¼‰
_model_infer_lock = asyncio.Lock()

# ======== è¿½åŠ : æ¨è«–çŠ¶æ…‹ç¢ºèªãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ========

async def _fetch_inference_status(client: httpx.AsyncClient) -> Optional[bool]:
    try:
        r = await client.get(f"{LLM_HOST}/inference_status")
        if r.status_code == 200:
            data = r.json()
            return bool(data.get("is_generating"))
    except Exception:
        pass
    return None

async def wait_until_inference_idle(timeout_sec: int = 180, poll_sec: float = 0.5) -> None:
    """
    /inference_status ã‚’ãƒãƒ¼ãƒªãƒ³ã‚°ã—ã¦ is_generating=False ã«ãªã‚‹ã¾ã§å¾…æ©Ÿã€‚
    ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãŸã‚‰504ã‚’æŠ•ã’ã‚‹ã€‚
    """
    deadline = time.time() + timeout_sec
    async with httpx.AsyncClient(timeout=10) as client:
        while time.time() < deadline:
            status = await _fetch_inference_status(client)
            # å–å¾—ã§ããªã‹ã£ãŸå ´åˆã¯å°ä¼‘æ­¢ã—ã¦å†è©¦è¡Œï¼ˆã‚µãƒ¼ãƒãŒè½ã¡ã¦ã„ã‚‹å¯èƒ½æ€§ã‚‚è€ƒæ…®ï¼‰
            if status is None:
                await asyncio.sleep(poll_sec)
                continue
            if status is False:
                return
            await asyncio.sleep(poll_sec)
    raise HTTPException(status_code=504, detail="inference wait timeout")

async def send_with_retry_nonstream(
    payload: dict,
    total_timeout_sec: int = 180,
    backoff_initial: float = 0.5,
    backoff_max: float = 3.0
) -> httpx.Response:
    """
    éã‚¹ãƒˆãƒªãƒ¼ãƒ å‘¼ã³å‡ºã—ã§ /v1/chat/completions ã‚’å©ãã€‚
    429(Model is currently generating) ã®é–“ã¯ /inference_status ã‚’è¦‹ãªãŒã‚‰ãƒªãƒˆãƒ©ã‚¤ã€‚
    """
    deadline = time.time() + total_timeout_sec
    backoff = backoff_initial
    async with httpx.AsyncClient(timeout=None) as client:
        while True:
            # ã¾ãšã‚µãƒ¼ãƒãŒç©ºãã¾ã§å¾…ã¤
            await wait_until_inference_idle(timeout_sec=max(1, int(deadline - time.time())))
            try:
                res = await client.post(f"{LLM_HOST}/v1/chat/completions", json=payload)
            except Exception as e:
                # ä¸€æ™‚çš„ãªæ¥ç¶šæ–­ã‚‚ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡
                if time.time() >= deadline:
                    raise HTTPException(status_code=504, detail=f"LLM request timeout: {e}")
                await asyncio.sleep(backoff)
                backoff = min(backoff * 1.6, backoff_max)
                continue

            if res.status_code == 429:
                if time.time() >= deadline:
                    raise HTTPException(status_code=504, detail="generation busy timeout")
                # å°‘ã—å¾…ã£ã¦å†è©¦è¡Œï¼ˆ/inference_status ã¯ wait_until_inference_idle å†…ã§è¦‹ã¦ã„ã‚‹ï¼‰
                await asyncio.sleep(backoff)
                backoff = min(backoff * 1.6, backoff_max)
                continue

            # 200ä»¥å¤–ã¯ãã®ã¾ã¾ã‚¨ãƒ©ãƒ¼
            res.raise_for_status()
            return res

async def stream_with_retry(
    payload: dict,
    total_timeout_sec: int = 180,
    poll_sec: float = 0.5
) -> StreamingResponse:
    """
    ã‚¹ãƒˆãƒªãƒ¼ãƒ å‘¼ã³å‡ºã—ã€‚é–‹å§‹å‰ã«ã‚µãƒ¼ãƒãŒç©ºãã¾ã§å¾…ã¤ã€‚
    é–‹å§‹æ™‚ã«429ã®å ´åˆã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¾ã§å¾…ã£ã¦ãƒªãƒˆãƒ©ã‚¤ã€‚
    """
    deadline = time.time() + total_timeout_sec

    async def do_stream():
        nonlocal deadline
        while True:
            # é–‹å§‹å‰å¾…æ©Ÿ
            await wait_until_inference_idle(timeout_sec=max(1, int(deadline - time.time())), poll_sec=poll_sec)
            try:
                async with httpx.AsyncClient(timeout=None) as client:
                    async with client.stream("POST", f"{LLM_HOST}/v1/chat/completions", json=payload) as res:
                        if res.status_code == 429:
                            if time.time() >= deadline:
                                raise HTTPException(status_code=504, detail="generation busy timeout")
                            # çŠ¶æ…‹ãŒç©ºãã¾ã§å†ãƒ«ãƒ¼ãƒ—
                            await asyncio.sleep(poll_sec)
                            continue
                        res.raise_for_status()

                        buffer = ""
                        async for line in res.aiter_lines():
                            if not line:
                                continue
                            if line.startswith("data: "):
                                data = line.replace("data: ", "").strip()
                                if data == "[DONE]":
                                    yield "data: [DONE]\n\n"
                                    break
                                try:
                                    delta = json.loads(data)["choices"][0]["delta"]
                                    content = delta.get("content", "")
                                    if content:
                                        buffer += content
                                    yield line + "\n"
                                except Exception:
                                    yield line + "\n"
                        return  # æ­£å¸¸çµ‚äº†
            except HTTPException:
                raise
            except Exception as e:
                # æ¥ç¶šç³»ã‚¨ãƒ©ãƒ¼ã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¾ã§å†è©¦è¡Œ
                if time.time() >= deadline:
                    raise HTTPException(status_code=504, detail=f"stream error timeout: {e}")
                await asyncio.sleep(poll_sec)
                continue

    return StreamingResponse(do_stream(), media_type="text/event-stream")

# ======== æ—¢å­˜ãƒ¢ãƒ‡ãƒ« ========

class Message(BaseModel):
    role: str
    content: str
    model: str = ""
    speaker_uuid: str = ""
    style_id: int = 0
    room_id: str = ""

class CompletionRequest(BaseModel):
    model: str = ""  # UIã‹ã‚‰æ¥ã¦ã‚‚ç„¡è¦–ï¼ˆconfigå„ªå…ˆï¼‰
    messages: List[Message]
    stream: bool = False
    speaker_uuid: str = ""
    style_id: int = 0
    room_id: str = ""
    prompt_id: str = "rag_default"
    rag_mode: str = "use"

def load_prompt_text(path: Path) -> str:
    if not path.exists():
        return ""
    lines = path.read_text(encoding="utf-8").splitlines()
    return "\n".join(lines[1:]).strip() if len(lines) > 1 else ""

def load_base_prompt(prompt_id: str) -> str:
    path = PROMPT_DIR / f"{prompt_id}.txt"
    if not path.exists():
        path = PROMPT_DIR / "rag_default.txt"
    return load_prompt_text(path)

def load_rag_instruction(rag_mode: str) -> str:
    path = RAG_PROMPT_DIR / f"{rag_mode}.txt"
    if not path.exists():
        path = RAG_PROMPT_DIR / "use.txt"
    return load_prompt_text(path)

def read_model_from_global_config() -> str:
    try:
        data = json.loads(GLOBAL_CONFIG_PATH.read_text(encoding="utf-8"))
        return (data.get("model") or "").strip()
    except Exception:
        return ""

async def get_llama_health() -> Optional[dict]:
    try:
        async with httpx.AsyncClient(timeout=10) as client:
            r = await client.get(f"{LLM_HOST}/health")
            if r.status_code == 200:
                return r.json()
    except Exception:
        pass
    return None

async def is_target_ready(target_model: str) -> bool:
    health = await get_llama_health()
    if not health:
        return False
    cur_model = (health.get("model") or "").strip()
    loaded = bool(health.get("model_loaded"))
    return (cur_model == target_model) and loaded

async def ensure_model_loaded_and_ready(target_model: str, timeout_sec: int = 180):
    """
    å¿…è¦æ™‚ã®ã¿ /load_model ã‚’å®Ÿè¡Œã—ã€/health ã§
    - model ãŒ target_model ã¨ä¸€è‡´
    - model_loaded ãŒ True
    ã«ãªã‚‹ã¾ã§å¾…æ©Ÿ
    """
    if not target_model:
        raise HTTPException(status_code=400, detail="ãƒ¢ãƒ‡ãƒ«æœªè¨­å®šï¼ˆglobal_config.jsonï¼‰")

    async with _model_load_lock:
        # æ—¢ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãªã‚‰å³è¿”ã™
        if await is_target_ready(target_model):
            return

        # ãƒ­ãƒ¼ãƒ‰è¦æ±‚
        async with httpx.AsyncClient(timeout=None) as client:
            resp = await client.post(f"{LLM_HOST}/load_model", json={"model": target_model})
            if resp.status_code >= 400:
                raise HTTPException(status_code=resp.status_code, detail=f"load_model failed: {resp.text}")

        # ãƒ­ãƒ¼ãƒ‰å®Œäº†å¾…ã¡
        deadline = time.time() + timeout_sec
        while time.time() < deadline:
            if await is_target_ready(target_model):
                return
            await asyncio.sleep(1)

        raise HTTPException(status_code=504, detail=f"model load timeout: {target_model}")

async def extract_keywords(query: str, model: str) -> List[str]:
    """
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚‚ LLM ã‚’å©ãã®ã§ã€ãã®å‰ã« ensure_model_loaded_and_ready ãŒ
    completions() ã§å‘¼ã°ã‚Œã¦ã„ã‚‹å‰æã€‚ã“ã“ã§ã‚‚ä¿é™ºã¨ã—ã¦ãƒ­ãƒ¼ãƒ‰ç¢ºèªã€‚
    """
    if not await is_target_ready(model):
        raise HTTPException(status_code=503, detail="ãƒ¢ãƒ‡ãƒ«ãŒã¾ã ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“")

    try:
        def clean_json_codeblock(text: str) -> str:
            match = re.search(r"```(?:json)?\s*(\[[\s\S]+?\])\s*```", text)
            if match:
                return match.group(1).strip()
            return text.strip()

        payload = {
            "model": model,
            "messages": [
                {
                    "role": "system",
                    "content": (
                        "ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•æ–‡ã«**å®Ÿéš›ã«ç™»å ´ã™ã‚‹åè©ã®ã¿**ã‚’æŠ½å‡ºã—ã€"
                        "æ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã—ã¦æœ€å¤§5å€‹ã¾ã§JSONé…åˆ—ã§è¿”ã™ã“ã¨ã§ã™ã€‚\n\n"
                        "ğŸ”’åˆ¶ç´„æ¡ä»¶ï¼š\n"
                        "- å‡ºåŠ›ã™ã‚‹å˜èªã¯ã€å…¥åŠ›æ–‡ã«**å®Ÿéš›ã«ç¾ã‚ŒãŸåè©**ã®ã¿ã¨ã™ã‚‹ã“ã¨\n"
                        "- åè©ä»¥å¤–ã¯å«ã‚ãªã„\n"
                        "- æŠ½è±¡èªãƒ»æ±ç”¨èªã¯å«ã‚ãªã„\n"
                        "- é †ç•ªã¯æ–‡ä¸­ã®å‡ºç¾é †ã¨ä¸€è‡´\n"
                        "- å‡ºåŠ›ã¯JSONé…åˆ—ã®ã¿ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ç¦æ­¢ï¼‰"
                    )
                },
                {"role": "user", "content": f"è³ªå•ï¼šã€Œ{query}ã€"},
            ],
            "stream": False,
        }

        async with httpx.AsyncClient() as client:
            res = await client.post(f"{LLM_HOST}/v1/chat/completions", json=payload, timeout=20)
            res.raise_for_status()

            content_raw = res.json()["choices"][0]["message"]["content"]
            content = clean_json_codeblock(content_raw)

            if not content or not content.startswith("["):
                logging.warning(f"[WARN] keywordæŠ½å‡º å¿œç­”å½¢å¼ç•°å¸¸: {content_raw}")
                return []
            return json.loads(content)

    except Exception as e:
        logging.warning(f"[WARN] keywordæŠ½å‡ºå¤±æ•—: {e}")
        return []

@router.post("/completions")
async def completions(req: CompletionRequest):
    selected_model = read_model_from_global_config()
    print(f"[INFO] ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«(from config): {selected_model}", flush=True)
    if not selected_model:
        raise HTTPException(status_code=400, detail="ãƒ¢ãƒ‡ãƒ«æœªè¨­å®šï¼ˆglobal_config.jsonï¼‰")

    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†ã¾ã§å¾…æ©Ÿ
    await ensure_model_loaded_and_ready(selected_model)

    user_message = next((m.content for m in reversed(req.messages) if m.role == "user"), "")
    if not user_message:
        raise HTTPException(status_code=400, detail="ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“")

    if req.room_id:
        save_streamed_message(req.room_id, role="user", content=user_message, model=selected_model)

    # æ¨è«–é †åºä¿è¨¼ï¼ˆã‚¢ãƒ—ãƒªå´ã§ç›´åˆ—åŒ–ï¼‰ï¼‹ ã‚µãƒ¼ãƒå´ã®is_generatingã‚‚ç›£è¦–
    async with _model_infer_lock:
        # ï¼ˆè¿½åŠ ï¼‰ã‚µãƒ¼ãƒãŒç©ºãã¾ã§äº‹å‰å¾…æ©Ÿ
        await wait_until_inference_idle(timeout_sec=180, poll_sec=0.5)

        character_raw = load_base_prompt(req.prompt_id).strip()
        rag_template_raw = load_rag_instruction(req.rag_mode)
        if "{context_text}" not in rag_template_raw:
            rag_template_raw += "\n\nã€RAGãƒãƒ£ãƒ³ã‚¯ã€‘\n{context_text}"

        context_text = ""
        if req.rag_mode != "off":
            keywords = await extract_keywords(user_message, selected_model)
            logging.info(f"[INFO] ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºçµæœï¼ˆ{len(keywords)}ä»¶ï¼‰: {', '.join(keywords)}")
            try:
                async with httpx.AsyncClient() as client:
                    res = await client.post(
                        SEARCH_API_URL,
                        json={"query": user_message, "keywords": keywords, "top_k": 50}
                    )
                    res.raise_for_status()
                    context_text = res.json().get("context_text", "")
                    logging.info(f"[RAGãƒãƒ£ãƒ³ã‚¯å…ˆé ­500æ–‡å­—]\n{context_text[:500]}")
            except Exception as e:
                logging.warning(f"[WARN] context_textå–å¾—å¤±æ•—: {e}")
                context_text = ""

        rag_filled = rag_template_raw.replace("{context_text}", context_text.strip())
        user_prompt = (
            f"ã€ä¸Šä½ãƒ«ãƒ¼ãƒ«ã€‘\n{character_raw}\n\n"
            f"ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã€‘\n{user_message.strip()}\n\n"
            f"ã€ä¸‹ä½ãƒ«ãƒ¼ãƒ«ã€‘\n{rag_filled.strip()}"
        )
        logging.info(f"[PROMPTæ–‡å­—æ•°]: {len(user_prompt)}")

        # ===== æ—¥æœ¬èªå›ºå®šï¼ˆæœ€å°æ”¹ä¿®ï¼šmessagesã ã‘å·®ã—æ›¿ãˆï¼‰ =====
        system_preamble = (
            "ä»¥å¾Œã®å¿œç­”ã¯å³æ ¼ã«æ—¥æœ¬èªã®ã¿ã§è¡Œã†ã“ã¨ã€‚è‹±å˜èªãƒ»è‹±æ–‡ã¯å‡ºåŠ›ã—ãªã„ã€‚"
            "ã‚‚ã—æ··åœ¨ã—ãŸå ´åˆã¯å³åº§ã«è‡ªç„¶ãªæ—¥æœ¬èªã¸è¨€ã„æ›ãˆã‚‹ã“ã¨ã€‚"
        )
        messages = [
            {"role": "system", "content": system_preamble},
            {"role": "system", "content": character_raw},  # æ—¢å­˜ã‚­ãƒ£ãƒ©æŒ‡ç¤ºã‚‚systemã§é©ç”¨
            {"role": "user",   "content": user_prompt},    # RAGè¾¼ã¿æœ¬æ–‡ã¯ãã®ã¾ã¾
        ]

        payload = {
            "model": selected_model,
            "messages": messages,
            "stream": req.stream
        }

        try:
            if req.stream:
                # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã¯å°‚ç”¨ã®ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã§SSEã‚’è¿”ã™
                resp = await stream_with_retry(payload, total_timeout_sec=180, poll_sec=0.5)
                # ä¿å­˜ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ æ™‚ã¯ä¸‹æµã§ã¾ã¨ã‚ã¦ä¿å­˜æ¸ˆã¿ã€‚ã“ã“ã§ã¯ãã®ã¾ã¾è¿”ã™ï¼‰
                return resp
            else:
                # éã‚¹ãƒˆãƒªãƒ¼ãƒ ã¯429ãƒªãƒˆãƒ©ã‚¤è¾¼ã¿ã§å®Ÿè¡Œ
                res = await send_with_retry_nonstream(payload, total_timeout_sec=180)
                data = res.json()
                try:
                    content = data["choices"][0]["message"]["content"]
                    if req.room_id and content.strip():
                        save_streamed_message(req.room_id, "assistant", content.strip(), selected_model)
                except Exception:
                    pass
                return data

        except httpx.HTTPStatusError as e:
            logging.error(f"[ERROR] LLMå¿œç­”å¤±æ•—: {e.response.text}")
            raise HTTPException(status_code=e.response.status_code, detail=e.response.text)
        except HTTPException:
            raise
        except Exception as e:
            logging.error(f"[ERROR] LLMå¿œç­”å¤±æ•—: {e}")
            raise HTTPException(status_code=500, detail=f"Unexpected error: {str(e)}")








































