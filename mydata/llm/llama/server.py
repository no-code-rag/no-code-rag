from fastapi import FastAPI, Query
from pydantic import BaseModel
from llama_cpp import Llama
import os

app = FastAPI()

# === „É¢„Éá„É´„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çπ‰∏ÄË¶ß ===
MODEL_PATHS = {
    "shisa": "./models/shisa8b-q4.gguf",
}

# === „É¢„Éá„É´„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÅÆ„Ç≠„É£„ÉÉ„Ç∑„É• ===
MODELS = {}

def get_model(model_name: str) -> Llama:
    if model_name not in MODELS:
        if model_name not in MODEL_PATHS:
            raise ValueError(f"Unknown model: {model_name}")
        print(f"üîÑ Loading model: {model_name}")
        MODELS[model_name] = Llama(
            model_path=MODEL_PATHS[model_name],
            n_ctx=4096,
            n_threads=os.cpu_count(),  # „Åô„Åπ„Å¶„ÅÆCPU„Çπ„É¨„ÉÉ„Éâ‰Ωø„ÅÜ
            n_batch=64
        )
    return MODELS[model_name]

# === „É™„ÇØ„Ç®„Çπ„ÉàÂΩ¢Âºè ===
class CompletionRequest(BaseModel):
    prompt: str
    model: str = "shisa"
    max_tokens: int = 128
    temperature: float = 0.7

# === „É¨„Çπ„Éù„É≥„ÇπÂΩ¢Âºè ===
class CompletionResponse(BaseModel):
    response: str

@app.post("/v1/completions", response_model=CompletionResponse)
def complete(req: CompletionRequest):
    llm = get_model(req.model)

    full_prompt = f"{req.prompt}"
    print(f"üìù {req.model}: {req.prompt}")

    result = llm(
        full_prompt,
        max_tokens=req.max_tokens,
        temperature=req.temperature,
        echo=False
    )

    text = result["choices"][0]["text"]
    return {"response": text.strip()}

