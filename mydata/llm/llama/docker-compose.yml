version: '3.8'

services:
  llama-fastapi:
    image: llama   # 既にビルド済みのローカルイメージ名
    container_name: llama
    environment:
      - MODEL=None
      - LLAMA_CPP_THREADS=12
      - LLAMA_CPP_CONTEXT_SIZE=4096
    volumes:
      - /mydata/llm/llama/models:/models
      - /mydata/llm/llama/server.py:/app/server.py
    networks:
      - secretary-net
    ports:
      - "8003:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  secretary-net:
    external: true

